"use strict";(self.webpackChunkphysical_ai_and_robotics=self.webpackChunkphysical_ai_and_robotics||[]).push([[8633],{4220:(e,o,t)=>{t.r(o),t.d(o,{assets:()=>c,contentTitle:()=>s,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"module-vla/try-with-ai/ai-voice-to-action","title":"Try with AI: \\"Voice-to-Action\\"","description":"This section provides curated prompts for you to explore Voice-to-Action in robotics further with an AI assistant. Use these prompts with your favorite Large Language Model (LLM) to deepen your understanding or tackle system design challenges.","source":"@site/docs/04-module-vla/try-with-ai/01-ai-voice-to-action.md","sourceDirName":"04-module-vla/try-with-ai","slug":"/module-vla/try-with-ai/ai-voice-to-action","permalink":"/ur/docs/module-vla/try-with-ai/ai-voice-to-action","draft":false,"unlisted":false,"editUrl":"https://github.com/AreebaZafarChohan/physical-ai-and-robotics/tree/main/frontend/docs/04-module-vla/try-with-ai/01-ai-voice-to-action.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"title":"Try with AI: \\"Voice-to-Action\\""},"sidebar":"tutorialSidebar","previous":{"title":"Quiz: \\"GPT Integration for Robot Control\\"","permalink":"/ur/docs/module-vla/quizzes/quiz-gpt-integration"},"next":{"title":"Try with AI: \\"Cognitive Planning with Large Language Models (LLMs)\\"","permalink":"/ur/docs/module-vla/try-with-ai/ai-cognitive-planning-llms"}}');var n=t(4848),a=t(8453);const r={title:'Try with AI: "Voice-to-Action"'},s="Try with AI: Voice-to-Action: Enabling Conversational Robotics",c={},l=[{value:"Prompt 1: Designing a Dialogue Flow for Clarifying Ambiguous Commands",id:"prompt-1-designing-a-dialogue-flow-for-clarifying-ambiguous-commands",level:2},{value:"Prompt 2: Cloud-Based vs. Edge ASR for Robotics",id:"prompt-2-cloud-based-vs-edge-asr-for-robotics",level:2},{value:"Prompt 3: Grounding Spoken Commands with Visual Perception",id:"prompt-3-grounding-spoken-commands-with-visual-perception",level:2}];function d(e){const o={code:"code",h1:"h1",h2:"h2",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,a.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(o.header,{children:(0,n.jsx)(o.h1,{id:"try-with-ai-voice-to-action-enabling-conversational-robotics",children:"Try with AI: Voice-to-Action: Enabling Conversational Robotics"})}),"\n",(0,n.jsx)(o.p,{children:"This section provides curated prompts for you to explore Voice-to-Action in robotics further with an AI assistant. Use these prompts with your favorite Large Language Model (LLM) to deepen your understanding or tackle system design challenges."}),"\n",(0,n.jsx)(o.hr,{}),"\n",(0,n.jsx)(o.h2,{id:"prompt-1-designing-a-dialogue-flow-for-clarifying-ambiguous-commands",children:"Prompt 1: Designing a Dialogue Flow for Clarifying Ambiguous Commands"}),"\n",(0,n.jsxs)(o.p,{children:[(0,n.jsx)(o.strong,{children:"Goal"}),": Understand how robots can handle ambiguity in spoken language."]}),"\n",(0,n.jsxs)(o.p,{children:[(0,n.jsx)(o.strong,{children:"AI Prompt"}),":\n\"You are designing a Voice-to-Action system for a humanoid robot operating in a kitchen. The robot receives the command, 'Pick up the cup.' However, there are two cups on the table: a red one and a blue one. Design a dialogue flow (including the robot's internal state changes and its spoken responses) that allows the robot to ask a clarifying question to resolve this ambiguity. Consider the ASR, NLU, and TTS components of the pipeline. Provide an example interaction script.\""]}),"\n",(0,n.jsx)(o.hr,{}),"\n",(0,n.jsx)(o.h2,{id:"prompt-2-cloud-based-vs-edge-asr-for-robotics",children:"Prompt 2: Cloud-Based vs. Edge ASR for Robotics"}),"\n",(0,n.jsxs)(o.p,{children:[(0,n.jsx)(o.strong,{children:"Goal"}),": Compare and contrast different ASR deployment strategies."]}),"\n",(0,n.jsxs)(o.p,{children:[(0,n.jsx)(o.strong,{children:"AI Prompt"}),':\n"Discuss the trade-offs between using cloud-based Automatic Speech Recognition (ASR) services (e.g., Google Cloud Speech-to-Text, Azure Speech) and edge-based ASR solutions (e.g., NVIDIA Riva, local Kaldi implementations) for a humanoid robot. Consider factors such as:']}),"\n",(0,n.jsxs)(o.ol,{children:["\n",(0,n.jsx)(o.li,{children:"Latency"}),"\n",(0,n.jsx)(o.li,{children:"Privacy/Security"}),"\n",(0,n.jsx)(o.li,{children:"Computational requirements"}),"\n",(0,n.jsx)(o.li,{children:"Offline capability"}),"\n",(0,n.jsx)(o.li,{children:'Cost\nFor a humanoid intended for home assistance, which approach would you recommend and why?"'}),"\n"]}),"\n",(0,n.jsx)(o.hr,{}),"\n",(0,n.jsx)(o.h2,{id:"prompt-3-grounding-spoken-commands-with-visual-perception",children:"Prompt 3: Grounding Spoken Commands with Visual Perception"}),"\n",(0,n.jsxs)(o.p,{children:[(0,n.jsx)(o.strong,{children:"Goal"}),": Explore how NLU integrates with visual information."]}),"\n",(0,n.jsxs)(o.p,{children:[(0,n.jsx)(o.strong,{children:"AI Prompt"}),":\n\"Explain how a Natural Language Understanding (NLU) system in a Voice-to-Action pipeline can 'ground' abstract spoken commands (e.g., 'move to the left of the chair') with real-time visual perception data from the robot's cameras. Describe the types of visual information (e.g., object detection, semantic segmentation, 3D pose estimation) that would be crucial for this grounding. Provide a conceptual example of an NLU output that incorporates visual data to create a robot-executable command.\""]}),"\n",(0,n.jsx)(o.pre,{children:(0,n.jsx)(o.code,{})})]})}function h(e={}){const{wrapper:o}={...(0,a.R)(),...e.components};return o?(0,n.jsx)(o,{...e,children:(0,n.jsx)(d,{...e})}):d(e)}},8453:(e,o,t)=>{t.d(o,{R:()=>r,x:()=>s});var i=t(6540);const n={},a=i.createContext(n);function r(e){const o=i.useContext(a);return i.useMemo(function(){return"function"==typeof e?e(o):{...o,...e}},[o,e])}function s(e){let o;return o=e.disableParentContext?"function"==typeof e.components?e.components(n):e.components||n:r(e.components),i.createElement(a.Provider,{value:o},e.children)}}}]);