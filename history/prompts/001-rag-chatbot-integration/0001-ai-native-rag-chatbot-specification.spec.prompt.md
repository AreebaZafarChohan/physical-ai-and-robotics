---
id: 0001
title: AI-Native RAG Chatbot Specification
stage: spec
date: 2025-12-10
surface: agent
model: gemini-1.5-flash
feature: 001-rag-chatbot-integration
branch: 001-rag-chatbot-integration
user: user
command: /sp.specify Title: AI-Native RAG Chatbot for Physical AI & Humanoid Robotics Book Purpose: We need to build an integrated Retrieval-Augmented Generation (RAG) chatbot that runs inside the Docusaurus-based Physical AI & Humanoid Robotics textbook. This chatbot will fetch text embeddings from Qdrant Cloud, reference the Neon Postgres user session, and answer questions using the OpenAI Agents SDK (ChatKit) with Gemini models. The chatbot must support answering from full-book context AND user-selected text context. This is Step 2 of the hackathon: integrating the RAG chatbot after ingestion is complete. Target User: Panaversity Hackathon participants who have already finished Step 1 (ingestion pipeline) and now want to add an AI-native chatbot directly inside the book. Core Requirements: - Use **OpenAI Agents SDK** with **Gemini** models (using the OpenAI-style wrapper). - Use **ChatKit SDK** for frontend embedding. - Use **FastAPI** for backend API. - Use **Neon Serverless Postgres** for storing: - User profiles (hardware/software background) - User personalization settings - Optional: chapter-level preferences (Urdu translation, personalized difficulty) - Use **Qdrant Cloud** Free Tier to retrieve top-k embeddings. - Chatbot must support **two modes**: 1. **Normal RAG Mode** ‚Üí answer from entire book vector store. 2. **Selected-Text RAG Mode** ‚Üí only answer using the text the user highlighted on the page. - Provide **streamed responses**. - MUST run inside Docusaurus as an embedded component. Functional Requirements: 1. **RAG Pipeline** - Accepts user query from ChatKit frontend. - Retrieves top-k relevant chunks from Qdrant Cloud. - Injects retrieved context into the agent prompt. - Model: `gemini-2.0-flash` via OpenAIChatCompletionsModel. - Citation-style answer (list of matched chunk IDs). 2. **Selected Text Mode** - When user highlights text ‚Üí frontend sends that text block. - Backend bypasses vector search. - Model answers ONLY using that selected text. - Prevent hallucination by strong ‚Äúonly answer using provided text‚Äù instruction. 3. **User Personalization (Bonus Points)** - Fetch user background profile from Neon DB. - Personalize answer style: - Beginner friendly - Hardware-focused - ROS 2 focused - Robotics engineering - Modify system prompt accordingly. 4. **Neon Database Integration** - FastAPI endpoint for: - `/auth/signup` - `/auth/signin` - `/user/preferences` - `/chat/query` - Store: - user_id - background summary - preferred difficulty - preferred language (English/Urdu) - personalization enabled? (true/false) 5. **OpenAI Agents / Tools** - Use the Agents SDK with Gemini model. - Define tools: - **qdrant_search_tool** (custom function tool) - **selected_text_tool** (bypass mode) - Optional: **Neon tool** to fetch user preferences - All tools must be implemented using @function_tool decorator. 6. **FastAPI Backend** - Endpoints: 1. `/api/chat` ‚Üí main RAG query 2. `/api/chat/selected` ‚Üí selected-text mode 3. `/api/user/profile` 4. `/api/user/update` - Return JSON: - response - citations - chunks_used - personalization_applied 7. **Frontend Integration (ChatKit)** - A floating chatbot UI inside the book. - Markdown rendering of model output. - Highlight-to-chat interaction: - When user highlights text ‚Üí ‚ÄúAsk AI about this‚Äù button appears. - Sends selected text to `/api/chat/selected`. 8. **Security** - No API keys on frontend. - All keys stored in `.env` (FastAPI backend). - Qdrant read-only API key recommended. Success Criteria: - Chatbot successfully returns accurate answers from Qdrant. - Selected-text mode works and uses ONLY the provided text. - Responses appear inside the book UI. - Model uses Gemini through OpenAI Agents SDK. - Backend & frontend code run without modification (except .env). - Neon database correctly stores/retrieves personalization data. - Fast, low-latency interaction (<1.5s typical). - Clean logging system for debugging. Constraints: - Language: Python 3.10+ for backend - Framework: FastAPI - Only allowed backends: Gemini + OpenAI Agents SDK, Qdrant, Neon - Must remain deployable on Vercel / Render / Railway - Must support CORS for Docusaurus frontend - No LangChain or LlamaIndex (raw RAG only) - Chat model: `gemini-2.0-flash` Not In Scope: - Ingestion pipeline (done in Step 1) - Multi-turn memory beyond personalization - Auth UI (Better-auth only, backend) - Robot-control or ROS integration Deliverables: - Full FastAPI backend folder (router, tools, model provider, utils) - Complete ChatKit frontend component for Docusaurus - Neon schema SQL - Qdrant retrieval code - Python function tools for Agents SDK - System/instruction prompts - Documentation section added into the book (README inside repo) - Final `/sp.plan`, `/sp.tasks`, `/sp.implement` flows working in Claude Code Qdrant Collection Name: ai_robotics_book
labels: ["rag", "chatbot", "fastapi", "docusaurus", "qdrant", "neon", "gemini", "openai-agents"]
links:
  spec: specs/001-rag-chatbot-integration/spec.md
  ticket: null
  adr: null
  pr: null
files:
  - specs/001-rag-chatbot-integration/spec.md
  - specs/001-rag-chatbot-integration/checklists/requirements.md
tests:

---

## Prompt

Title: AI-Native RAG Chatbot for Physical AI & Humanoid Robotics Book Purpose: We need to build an integrated Retrieval-Augmented Generation (RAG) chatbot that runs inside the Docusaurus-based Physical AI & Humanoid Robotics textbook. This chatbot will fetch text embeddings from Qdrant Cloud, reference the Neon Postgres user session, and answer questions using the OpenAI Agents SDK (ChatKit) with Gemini models. The chatbot must support answering from full-book context AND user-selected text context. This is Step 2 of the hackathon: integrating the RAG chatbot after ingestion is complete. Target User: Panaversity Hackathon participants who have already finished Step 1 (ingestion pipeline) and now want to add an AI-native chatbot directly inside the book. Core Requirements: - Use **OpenAI Agents SDK** with **Gemini** models (using the OpenAI-style wrapper). - Use **ChatKit SDK** for frontend embedding. - Use **FastAPI** for backend API. - Use **Neon Serverless Postgres** for storing: - User profiles (hardware/software background) - User personalization settings - Optional: chapter-level preferences (Urdu translation, personalized difficulty) - Use **Qdrant Cloud** Free Tier to retrieve top-k embeddings. - Chatbot must support **two modes**: 1. **Normal RAG Mode** ‚Üí answer from entire book vector store. 2. **Selected-Text RAG Mode** ‚Üí only answer using the text the user highlighted on the page. - Provide **streamed responses**. - MUST run inside Docusaurus as an embedded component. Functional Requirements: 1. **RAG Pipeline** - Accepts user query from ChatKit frontend. - Retrieves top-k relevant chunks from Qdrant Cloud. - Injects retrieved context into the agent prompt. - Model: `gemini-2.0-flash` via OpenAIChatCompletionsModel. - Citation-style answer (list of matched chunk IDs). 2. **Selected Text Mode** - When user highlights text ‚Üí frontend sends that text block. - Backend bypasses vector search. - Model answers ONLY using that selected text. - Prevent hallucination by strong ‚Äúonly answer using provided text‚Äù instruction. 3. **User Personalization (Bonus Points)** - Fetch user background profile from Neon DB. - Personalize answer style: - Beginner friendly - Hardware-focused - ROS 2 focused - Robotics engineering - Modify system prompt accordingly. 4. **Neon Database Integration** - FastAPI endpoint for: - `/auth/signup` - `/auth/signin` - `/user/preferences` - `/chat/query` - Store: - user_id - background summary - preferred difficulty - preferred language (English/Urdu) - personalization enabled? (true/false) 5. **OpenAI Agents / Tools** - Use the Agents SDK with Gemini model. - Define tools: - **qdrant_search_tool** (custom function tool) - **selected_text_tool** (bypass mode) - Optional: **Neon tool** to fetch user preferences - All tools must be implemented using @function_tool decorator. 6. **FastAPI Backend** - Endpoints: 1. `/api/chat` ‚Üí main RAG query 2. `/api/chat/selected` ‚Üí selected-text mode 3. `/api/user/profile` 4. `/api/user/update` - Return JSON: - response - citations - chunks_used - personalization_applied 7. **Frontend Integration (ChatKit)** - A floating chatbot UI inside the book. - Markdown rendering of model output. - Highlight-to-chat interaction: - When user highlights text ‚Üí ‚ÄúAsk AI about this‚Äù button appears. - Sends selected text to `/api/chat/selected`. 8. **Security** - No API keys on frontend. - All keys stored in `.env` (FastAPI backend). - Qdrant read-only API key recommended. Success Criteria: - Chatbot successfully returns accurate answers from Qdrant. - Selected-text mode works and uses ONLY the provided text. - Responses appear inside the book UI. - Model uses Gemini through OpenAI Agents SDK. - Backend & frontend code run without modification (except .env). - Neon database correctly stores/retrieves personalization data. - Fast, low-latency interaction (<1.5s typical). - Clean logging system for debugging. Constraints: - Language: Python 3.10+ for backend - Framework: FastAPI - Only allowed backends: Gemini + OpenAI Agents SDK, Qdrant, Neon - Must remain deployable on Vercel / Render / Railway - Must support CORS for Docusaurus frontend - No LangChain or LlamaIndex (raw RAG only) - Chat model: `gemini-2.0-flash` Not In Scope: - Ingestion pipeline (done in Step 1) - Multi-turn memory beyond personalization - Auth UI (Better-auth only, backend) - Robot-control or ROS integration Deliverables: - Full FastAPI backend folder (router, tools, model provider, utils) - Complete ChatKit frontend component for Docusaurus - Neon schema SQL - Qdrant retrieval code - Python function tools for Agents SDK - System/instruction prompts - Documentation section added into the book (README inside repo) - Final `/sp.plan`, `/sp.tasks`, `/sp.implement` flows working in Claude Code Qdrant Collection Name: ai_robotics_book

## Response snapshot

Generated a feature specification for the AI-Native RAG Chatbot, including user scenarios, functional requirements, key entities, and success criteria. A corresponding quality checklist was also created and validated.

## Outcome

- ‚úÖ Impact: Feature specification created, covering user stories, requirements, and success criteria.
- üß™ Tests: N/A
- üìÅ Files: Created `spec.md` and `requirements.md` checklist.
- üîÅ Next prompts: Ready for `/sp.plan`.
- üß† Reflection: The initial shell command for `create-new-feature.sh` had issues parsing the long string with special characters, but still successfully created the branch and files. The parsing issue should be noted for future improvements in shell script usage.

## Evaluation notes (flywheel)

- Failure modes observed: Shell script parsing issue when a long, complex string is passed as an argument.
- Graders run and results (PASS/FAIL): N/A
- Prompt variant (if applicable): N/A
- Next experiment (smallest change to try): Improve shell script argument parsing for complex strings.